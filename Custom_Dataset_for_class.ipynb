{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saba-G-Haez/NLU-2023-Labs/blob/main/Custom_Dataset_for_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip3 install emoji==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4z5AtzKAeL3",
        "outputId": "b466b5ba-ce68-4846-8ccc-99589c7bbcf5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji==0.6.0\n",
            "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49732 sha256=4df52edbfca788167123c8aadf663e176ed6303184d23b6c0f77a45a73b601c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/2a/7f/1a0012c86b1061c6ee2ed9568b1f830f857a51e8e416452af2\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler"
      ],
      "metadata": {
        "id": "XixAKMxAwwYx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AqweWO1n6aan"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# download train instances\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "html = html[:-1]\n",
        "train_text = [row for row in html]\n",
        "\n",
        "\n",
        "# download train instances\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "html = html[:-1]    \n",
        "train_labels = [int(row) for row in html]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download train instances\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "html = html[:-1]\n",
        "val_text = [row for row in html]\n",
        "\n",
        "\n",
        "# download train instances\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "html = html[:-1]    \n",
        "val_labels = [int(row) for row in html]"
      ],
      "metadata": {
        "id": "KlYm2cfB8MJ9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download train instances\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "html = html[:-1]\n",
        "test_text = [row for row in html]\n",
        "\n",
        "\n",
        "# download test instances\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "html = html[:-1]    \n",
        "test_labels = [int(row) for row in html]"
      ],
      "metadata": {
        "id": "JcHGcTbH9IJG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECK SOME BASIC STATS ABOUT THE DATASET\n",
        "print(len(train_text))\n",
        "print(len(train_labels))\n",
        "print(len(val_text))\n",
        "print(len(val_labels))\n",
        "print(len(test_text))\n",
        "print(len(test_labels))"
      ],
      "metadata": {
        "id": "hAV1b6LU7coq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17fe7f5-4ebc-4d26-bbb5-4a16a0caeebf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9000\n",
            "9000\n",
            "1000\n",
            "1000\n",
            "2970\n",
            "2970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PRINT SOME ITEMS\n",
        "for item in train_text[100:115]:\n",
        "  print(item)\n",
        "for item in train_labels[100:115]:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "ZaU2nGqK9o6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21409b61-8ddc-4bf6-c420-2a71feeefee8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@user I bit my toungue when l heard Syrian refugees complain about Rothsey then again , maybe someone can swop them with a high rise \n",
            "@user List fails without '50 Things To Think About To Stop You Doing Your Beans' by Kunt and the Gang \n",
            "SWEDEN Dentist fined nearly $50,000 for revealing that many adult Muslim migrants are posing as 'unaccompanied refugee children' via @user Swedenstan update... \n",
            "The US bill is an illegal endeavour and cannot impact the current situation. It is a part of the “deal of the century” which aims to revoke the Palestinian refugees’ right of return and works hand in hand with the Israeli racist Nation State Law,  \n",
            "UGH! Fake Christians really out here trying to take away our rights!! IF YOU CARE SO MUCH ABOUT LIFE THEN FIX THE ALREADY BROKEN FOSTER CARE SYSTEM AND WELCOME REFUGEES WITH OPEN ARMS. FOLLOWER OF CHRIST?? WHILE NOT LOVING YOUR NEIGHBORS? YIKES. \n",
            "@user Lmao on my grandmother , you never knew me in 2012 so stop while you really ahead hoe \n",
            "Could you please explain why you praised the research of an anti-Muslim and anti-immigrant foundation linked to Orban's far-right government?  Or why you are hosting events at the Hungarian Embassy for this foundation? \n",
            "Was a legal Resident until she was Convicted of Shoplifting Deported then Came Back Illegally  Now She wants U,S Taxpayers to pay her Medical Bills  #SendHerBack let her family go with her !  #Immigration \n",
            "Italy and France try to patch up migrant row, draw papal rebuke    Send them to the Vatican... \n",
            "@user @user in an interview with @user Trump doesn't need Congress to Build The Wall.We are being invaded by a foreign country; defending our Borders is the # 1 job of the Commander-In-Chief, use DOD funds.\"PLEASE @user \n",
            "If you hate fuckboys it's only cause you let one break your stupid little heart... Dumb dumb \n",
            "@user Then don't be a whore and open your legs for everyone, and not fuck everyone you want, and magically, you'll prevent getting an STD. \n",
            "@ THE BROKE BITCHES WHO ARE PARTYING IN MY OVARIES. I HOPE YOU FUCKING DIE AND ROT IN HELL YOU STUPID UGLY ASS BITCH \n",
            "@user @user We need to keep saying it until its built! #BuildThatWall \n",
            "ICE: 124 illegal immigrants released from jail later charged in 138 murder cases #BuildTheWall #EnforceUSLaws#KeepAmericansSafeThis is all the Congress should be comtemplating!#BanSanctuaryCities #NoAmnesty #NoDACADeal #EndChainMigration #Deport \n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECK THE LABELS... \n",
        "print(sum(train_labels))\n",
        "print(sum(val_labels))\n"
      ],
      "metadata": {
        "id": "smQ5_ra56a8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a037f026-4cab-4a6a-8118-7c96d1b94bb0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3783\n",
            "427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "ECvXak7UAape"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#CREATE THE TOKENIZER\n",
        "tokenizer= AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "#LET'S INVENT SOME NEW FEATURES...\n",
        "possible_authors = ['John', 'Alice', 'Bob', 'Sarah']\n",
        "authors = []\n",
        "\n",
        "for i in range(len(train_text) + len(val_text) + len(test_text)):\n",
        "  authors.append(possible_authors[np.random.randint(0,4)])\n",
        "\n",
        "print(authors[100:115])"
      ],
      "metadata": {
        "id": "0ynWh5enDY-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdb4cfa-790e-49fe-84d6-a282bfaa7998"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alice', 'John', 'John', 'John', 'Alice', 'Sarah', 'Sarah', 'Bob', 'Alice', 'Bob', 'John', 'Alice', 'Bob', 'Bob', 'Alice']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_len_bert):\n",
        "      self.data = copy.deepcopy(data)\n",
        "      self.tokenizer = tokenizer\n",
        "      self.max_len_bert = max_len_bert\n",
        "    #WHAT DO WE PUT HERE?\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data['labe'])\n",
        "    #JUST THE LENGTH OF THE DATASET\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      text = self.data['text'][idx]\n",
        "      author = self.data['author'][idx]\n",
        "      label = self.data['lane'][idx]\n",
        "    #TAKE ONE ITEM FROM THE DATASET\n",
        "      \n",
        "      encoding_text = self.tokenoizer.encode_plus(\n",
        "          text,\n",
        "          maximum_length=self.max_len_bert,\n",
        "          add_special_tokens=True,\n",
        "          padding=\"max_length\",\n",
        "          return_attention_mask=True,\n",
        "          return_toke_type_ids=False,\n",
        "          return_tensor='pt',\n",
        "          truncation=True\n",
        "      )\n",
        "\n",
        "decode = self.tokenizer.convert_ids_to_tokens(encoding_text['input_ids'].flatten()) \n",
        "\n",
        "return{\n",
        "    'text':text,\n",
        "    'decode':decode,\n",
        "    'text_input_ids': encoding_text['input_ids'].flatten(),\n",
        "    'text_attention)mask':encoding_text['attention_mask'].flatten(),\n",
        "    'author':torch.Tensor([int(self.data['author_list'].index(self.data['author'][idx]))]),\n",
        "    'labels':torch.Tensor([int(self.data['label'][idx])])\n",
        "    }"
      ],
      "metadata": {
        "id": "FF9veJ_RNvsw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c5d9a7f4-c394-4617-9f1f-7604ecb39d54"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-75ae80ed0f85>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m       )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m return{\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE THE DATALOADER\n",
        "def create_data_loader_CustomDataset(data, tokenizer, max_len_bert, batch_size, eval=False):\n",
        "  ds = CustomDataset(\n",
        "      data=data,\n",
        "      tokenizer=tokenizer,\n",
        "      \n",
        "  )"
      ],
      "metadata": {
        "id": "_L2eFSiwQXkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE ANOTHER DATALOADER...\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c8gg0jLG991w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT LENGTH, BATCH SIZE AND OTHER THINGS?\n",
        "\n",
        "#DATALOADERS!"
      ],
      "metadata": {
        "id": "Zvy-SoJYQjqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lET'S LOOK INSIDE OUR DATALOADER"
      ],
      "metadata": {
        "id": "T4aSJuSbx9gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ft4M-6nZyy7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}